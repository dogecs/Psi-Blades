{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pemodelan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lib\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi Sentimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source CODE dari\n",
    "https://github.com/commitunuja/analisis-sentimen-naive-bayes-tf-idf/blob/master/.ipynb_checkpoints/Untitled-checkpoint.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run S_4_Analisis_sentimen.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier in Python KAGGLE\n",
    "https://www.kaggle.com/prashant111/naive-bayes-classifier-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "datax = pd.read_csv('E:\\Program\\[2] Program\\AnSent\\Data\\Data_5_Lex_sen\\Lex_Indihome.csv')\n",
    "\n",
    "# menghapus kolom\n",
    "del datax['Unnamed: 0.1']\n",
    "del datax['Unnamed: 0']\n",
    "# datas = datax.drop(['Unnamed: 0.1', 'Unnamed: 0', 'Data Teks', 'Data Teks_CaseFolding', 'Data Teks_clean_alay', 'Data Teks_Token', 'Data Teks_Stopword', 'Jumlah_polarity'])\n",
    "# data.rename( columns={'Unnamed: 0':'Indeks'}, inplace=True )\n",
    "data = datax.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.astype({'Sentimen' : 'category'})\n",
    "# data = data.astype({'Data Teks_Stemming' : 'string'})\n",
    "# data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sudah', 'dilakukan', 'sesuai', 'pesan', 'belum', 'bisa']\n",
      "\n",
      "type :  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# convert list formated string to list\n",
    "import ast\n",
    "import numpy as np\n",
    "index = 0\n",
    "\n",
    "def convert_text_list(texts):\n",
    "    texts = ast.literal_eval(texts)\n",
    "    return [text for text in texts]\n",
    "\n",
    "data['Data Teks_List'] = data['Data Teks_Token'].apply(convert_text_list)\n",
    "\n",
    "\n",
    "print(data['Data Teks_List'][index])\n",
    "\n",
    "print('\\ntype : ', type(data['Data Teks_List']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'sudah': 0.16666666666666666, 'dilakukan': 0....\n",
       "1                                           {'p': 1.0}\n",
       "2    {'kak': 0.041666666666666664, 'dila': 0.041666...\n",
       "3    {'ini': 0.14285714285714285, 'kapan': 0.071428...\n",
       "4    {'oy': 0.08333333333333333, 'mola': 0.08333333...\n",
       "Name: TF_dict, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_TF(document):\n",
    "    # Counts the number of times the word appears in review\n",
    "    TF_dict = {}\n",
    "    for term in document:\n",
    "        if term in TF_dict:\n",
    "            TF_dict[term] += 1\n",
    "        else:\n",
    "            TF_dict[term] = 1\n",
    "    # Computes tf for each word\n",
    "    for term in TF_dict:\n",
    "        TF_dict[term] = TF_dict[term] / len(document)\n",
    "    return TF_dict\n",
    "\n",
    "data[\"TF_dict\"] = data['Data Teks_List'].apply(calc_TF)\n",
    "\n",
    "data[\"TF_dict\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                term \t TF\n",
      "\n",
      "               sudah \t 0.16666666666666666\n",
      "           dilakukan \t 0.16666666666666666\n",
      "              sesuai \t 0.16666666666666666\n",
      "               pesan \t 0.16666666666666666\n",
      "               belum \t 0.16666666666666666\n",
      "                bisa \t 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('%20s' % \"term\", \"\\t\", \"TF\\n\")\n",
    "for key in data[\"TF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", data[\"TF_dict\"][index][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_DF(tfDict):\n",
    "    count_DF = {}\n",
    "    # Run through each document's tf dictionary and increment countDict's (term, doc) pair\n",
    "    for document in tfDict:\n",
    "        for term in document:\n",
    "            if term in count_DF:\n",
    "                count_DF[term] += 1\n",
    "            else:\n",
    "                count_DF[term] = 1\n",
    "    return count_DF\n",
    "\n",
    "DF = calc_DF(data[\"TF_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_document = len(data)\n",
    "\n",
    "def calc_IDF(__n_document, __DF):\n",
    "    IDF_Dict = {}\n",
    "    for term in __DF:\n",
    "        IDF_Dict[term] = np.log(__n_document / (__DF[term] + 1))\n",
    "    return IDF_Dict\n",
    "  \n",
    "#Stores the idf dictionary\n",
    "IDF = calc_IDF(n_document, DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc TF-IDF\n",
    "def calc_TF_IDF(TF):\n",
    "    TF_IDF_Dict = {}\n",
    "    #For each word in the review, we multiply its tf and its idf.\n",
    "    for key in TF:\n",
    "        TF_IDF_Dict[key] = TF[key] * IDF[key]\n",
    "    return TF_IDF_Dict\n",
    "\n",
    "#Stores the TF-IDF Series\n",
    "data[\"TF-IDF_dict\"] = data[\"TF_dict\"].apply(calc_TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                term \t         TF \t              TF-IDF\n",
      "\n",
      "               sudah \t 0.16666666666666666 \t 0.2542200919300053\n",
      "           dilakukan \t 0.16666666666666666 \t 0.6251534177936807\n",
      "              sesuai \t 0.16666666666666666 \t 0.8623381401674486\n",
      "               pesan \t 0.16666666666666666 \t 0.2018062581670243\n",
      "               belum \t 0.16666666666666666 \t 0.5516581183661684\n",
      "                bisa \t 0.16666666666666666 \t 0.31884197335297587\n"
     ]
    }
   ],
   "source": [
    "print('%20s' % \"term\", \"\\t\", '%10s' % \"TF\", \"\\t\", '%20s' % \"TF-IDF\\n\")\n",
    "for key in data[\"TF-IDF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", data[\"TF_dict\"][index][key] ,\"\\t\" , data[\"TF-IDF_dict\"][index][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print first row matrix TF_IDF_Vec Series\n",
      "\n",
      "[0.0, 0.0, 0.2018062581670243, 0.0, 0.0, 0.0, 0.2542200919300053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31884197335297587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "matrix size :  50\n"
     ]
    }
   ],
   "source": [
    "# sort descending by value for DF dictionary \n",
    "sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)[:50]\n",
    "\n",
    "# Create a list of unique words from sorted dictionay `sorted_DF`\n",
    "unique_term = [item[0] for item in sorted_DF]\n",
    "\n",
    "def calc_TF_IDF_Vec(__TF_IDF_Dict):\n",
    "    TF_IDF_vector = [0.0] * len(unique_term)\n",
    "\n",
    "    # For each unique word, if it is in the review, store its TF-IDF value.\n",
    "    for i, term in enumerate(unique_term):\n",
    "        if term in __TF_IDF_Dict:\n",
    "            TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "    return TF_IDF_vector\n",
    "\n",
    "data[\"TF_IDF_Vec\"] = data[\"TF-IDF_dict\"].apply(calc_TF_IDF_Vec)\n",
    "\n",
    "print(\"print first row matrix TF_IDF_Vec Series\\n\")\n",
    "print(data[\"TF_IDF_Vec\"][0])\n",
    "\n",
    "print(\"\\nmatrix size : \", len(data[\"TF_IDF_Vec\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('E:\\Program\\[2] Program\\AnSent\\Data\\Data_8_Pemod\\Mod_Indihome.csv')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negatif    3880\n",
       "positif    2648\n",
       "netral      537\n",
       "Name: sentimen, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentimen'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBC\n",
    "https://github.com/rizkiamanullah/Analisis-Sentiment-Naive-Bayes/blob/main/rizkiamanullah-Analysis-Sentiment-NaiveB.ipynb\n",
    "\n",
    "# ini penting (confusion matrix)\n",
    "https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataframe\n",
    "data_s = datax.copy()\n",
    "# data_s.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus dataframe netral\n",
    "data_s = data_s[data_s.sentimen != 'netral']\n",
    "# data_s.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversi nilai\n",
    "def convernilai(data):\n",
    "    if data == 'positif':\n",
    "        return 1\n",
    "    elif data == 'negatif':\n",
    "        return -1\n",
    "\n",
    "data_s['label'] = data_s['sentimen'].apply(lambda x: convernilai(x))\n",
    "# data_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1107)\t0.2900543311693539\n",
      "  (0, 856)\t0.4342205774997905\n",
      "  (0, 6282)\t0.21951162489795123\n",
      "  (0, 7267)\t0.6219845912013412\n",
      "  (0, 1938)\t0.4777814385244335\n",
      "  (0, 7599)\t0.2529736881375842\n",
      "  (1, 8704)\t0.07233936411316859\n",
      "  (1, 2256)\t0.17331042627839335\n",
      "  (1, 7219)\t0.19222131111819674\n",
      "  (1, 7409)\t0.19022350604834604\n",
      "  (1, 1604)\t0.12006896804403233\n",
      "  (1, 1997)\t0.2992334941998413\n",
      "  (1, 6192)\t0.16610109104908377\n",
      "  (1, 3302)\t0.09380290684818213\n",
      "  (1, 6900)\t0.12283719506684926\n",
      "  (1, 7279)\t0.2030005397797552\n",
      "  (1, 8545)\t0.17577236509589403\n",
      "  (1, 3477)\t0.16310333297035656\n",
      "  (1, 7129)\t0.18698397515946064\n",
      "  (1, 2950)\t0.1388929994226607\n",
      "  (1, 2393)\t0.27004197817123304\n",
      "  (1, 5921)\t0.1422204488599797\n",
      "  (1, 3902)\t0.10717172470414077\n",
      "  (1, 1891)\t0.22228069379302487\n",
      "  (1, 8402)\t0.2992334941998413\n",
      "  :\t:\n",
      "  (6526, 3287)\t0.17112902908580527\n",
      "  (6526, 7841)\t0.36196408559362897\n",
      "  (6526, 7239)\t0.19212764998055495\n",
      "  (6526, 3885)\t0.17949269241597407\n",
      "  (6526, 7193)\t0.15899876421771159\n",
      "  (6526, 573)\t0.15940170606390866\n",
      "  (6526, 4540)\t0.13436905602248297\n",
      "  (6526, 6684)\t0.1673496454384273\n",
      "  (6526, 2879)\t0.10238670951580436\n",
      "  (6526, 7976)\t0.15860050784802682\n",
      "  (6526, 1596)\t0.1511564659849276\n",
      "  (6526, 7375)\t0.10204325598930554\n",
      "  (6526, 4784)\t0.12548088532250476\n",
      "  (6526, 1790)\t0.11867648119988172\n",
      "  (6526, 3233)\t0.10886978211646037\n",
      "  (6526, 8704)\t0.07191350142893428\n",
      "  (6526, 3682)\t0.06261637201215711\n",
      "  (6527, 748)\t0.4315020678883622\n",
      "  (6527, 4588)\t0.4143027981366049\n",
      "  (6527, 1864)\t0.42804798929538984\n",
      "  (6527, 8286)\t0.4676657472247255\n",
      "  (6527, 5267)\t0.2539801101885293\n",
      "  (6527, 81)\t0.2366706675069586\n",
      "  (6527, 8704)\t0.297292585341911\n",
      "  (6527, 7599)\t0.1769775185392641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "v_data= vectorizer.fit_transform(data_s['Data Teks_clean_alay'].values.astype('U'))\n",
    "print(v_data)\n",
    "v_data= vectorizer.fit_transform(data_s['Data Teks_clean_alay'].values.astype('U')).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyak data train : 5222\n",
      "Banyak data test  : 1306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(v_data, data_s['label'], test_size=0.2, random_state=0)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "print('Banyak data train :',len(X_train))\n",
    "print('Banyak data test  :',len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[357 170]\n",
      " [110 669]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.86      0.83       779\n",
      "           1       0.76      0.68      0.72       527\n",
      "\n",
      "    accuracy                           0.79      1306\n",
      "   macro avg       0.78      0.77      0.77      1306\n",
      "weighted avg       0.78      0.79      0.78      1306\n",
      "\n",
      "nilai akurasinya adalah 78.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_preds, labels=[1,-1]))\n",
    "print(classification_report(y_test,y_preds))\n",
    "print('nilai akurasinya adalah {:.2f}'.format((accuracy_score(y_test,y_preds)*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pemodelan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Tanggal</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Data Teks</th>\n",
       "      <th>Data Teks_CaseFolding</th>\n",
       "      <th>Data Teks_clean_alay</th>\n",
       "      <th>Data Teks_Token</th>\n",
       "      <th>Data Teks_Stopword</th>\n",
       "      <th>Data Teks_Stemming</th>\n",
       "      <th>jumlah_polarity</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25012</td>\n",
       "      <td>25012</td>\n",
       "      <td>2021-10-19 14:48:25+00:00</td>\n",
       "      <td>FirstMediaCares</td>\n",
       "      <td>@adoths  Selamat malam FIRST People, Mohon maa...</td>\n",
       "      <td>selamat malam first people mohon maaf atas ket...</td>\n",
       "      <td>selamat malam first people mohon maaf atas ket...</td>\n",
       "      <td>['selamat', 'malam', 'first', 'people', 'mohon...</td>\n",
       "      <td>['selamat', 'malam', 'first', 'people', 'mohon...</td>\n",
       "      <td>selamat malam first people mohon maaf ketidakn...</td>\n",
       "      <td>-10</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1784</td>\n",
       "      <td>1784</td>\n",
       "      <td>2021-10-30 16:40:56+00:00</td>\n",
       "      <td>FirstMediaCares</td>\n",
       "      <td>@DanielVHD Mohon check DM anda.</td>\n",
       "      <td>mohon check dm anda</td>\n",
       "      <td>mohon  memeriksa  pesan kamu</td>\n",
       "      <td>['mohon', 'memeriksa', 'pesan', 'kamu']</td>\n",
       "      <td>['mohon', 'memeriksa', 'pesan']</td>\n",
       "      <td>mohon memeriksa pesan</td>\n",
       "      <td>-6</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37330</td>\n",
       "      <td>37330</td>\n",
       "      <td>2021-10-13 05:30:28+00:00</td>\n",
       "      <td>FirstMediaCares</td>\n",
       "      <td>@akhyaroy Hi First People. Kami sudah merespon...</td>\n",
       "      <td>hi first people kami sudah merespon via dm sil...</td>\n",
       "      <td>hi first people kami sudah merespon via  pesan...</td>\n",
       "      <td>['hi', 'first', 'people', 'kami', 'sudah', 'me...</td>\n",
       "      <td>['hi', 'first', 'people', 'merespon', 'via', '...</td>\n",
       "      <td>hi first people merespon via pesan silakan dic...</td>\n",
       "      <td>8</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2021-10-30 05:43:35+00:00</td>\n",
       "      <td>FirstMediaCares</td>\n",
       "      <td>@soeguss Adapun estimasi perbaikan hingga 30 O...</td>\n",
       "      <td>adapun estimasi perbaikan hingga  oktober  puk...</td>\n",
       "      <td>adapun estimasi perbaikan hingga  oktober  puk...</td>\n",
       "      <td>['adapun', 'estimasi', 'perbaikan', 'hingga', ...</td>\n",
       "      <td>['estimasi', 'perbaikan', 'oktober', 'wib', 'i...</td>\n",
       "      <td>estimasi perbaikan oktober wib informasi probl...</td>\n",
       "      <td>-5</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8225</td>\n",
       "      <td>8225</td>\n",
       "      <td>2021-10-28 08:20:58+00:00</td>\n",
       "      <td>DediBudiawan2</td>\n",
       "      <td>@FirstMediaCares Baik tks</td>\n",
       "      <td>baik tks</td>\n",
       "      <td>baik tks</td>\n",
       "      <td>['baik', 'tks']</td>\n",
       "      <td>['tks']</td>\n",
       "      <td>tks</td>\n",
       "      <td>2</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                    Tanggal          User ID  \\\n",
       "0       25012         25012  2021-10-19 14:48:25+00:00  FirstMediaCares   \n",
       "1        1784          1784  2021-10-30 16:40:56+00:00  FirstMediaCares   \n",
       "2       37330         37330  2021-10-13 05:30:28+00:00  FirstMediaCares   \n",
       "3        3000          3000  2021-10-30 05:43:35+00:00  FirstMediaCares   \n",
       "4        8225          8225  2021-10-28 08:20:58+00:00    DediBudiawan2   \n",
       "\n",
       "                                           Data Teks  \\\n",
       "0  @adoths  Selamat malam FIRST People, Mohon maa...   \n",
       "1                    @DanielVHD Mohon check DM anda.   \n",
       "2  @akhyaroy Hi First People. Kami sudah merespon...   \n",
       "3  @soeguss Adapun estimasi perbaikan hingga 30 O...   \n",
       "4                          @FirstMediaCares Baik tks   \n",
       "\n",
       "                               Data Teks_CaseFolding  \\\n",
       "0  selamat malam first people mohon maaf atas ket...   \n",
       "1                                mohon check dm anda   \n",
       "2  hi first people kami sudah merespon via dm sil...   \n",
       "3  adapun estimasi perbaikan hingga  oktober  puk...   \n",
       "4                                           baik tks   \n",
       "\n",
       "                                Data Teks_clean_alay  \\\n",
       "0  selamat malam first people mohon maaf atas ket...   \n",
       "1                       mohon  memeriksa  pesan kamu   \n",
       "2  hi first people kami sudah merespon via  pesan...   \n",
       "3  adapun estimasi perbaikan hingga  oktober  puk...   \n",
       "4                                           baik tks   \n",
       "\n",
       "                                     Data Teks_Token  \\\n",
       "0  ['selamat', 'malam', 'first', 'people', 'mohon...   \n",
       "1            ['mohon', 'memeriksa', 'pesan', 'kamu']   \n",
       "2  ['hi', 'first', 'people', 'kami', 'sudah', 'me...   \n",
       "3  ['adapun', 'estimasi', 'perbaikan', 'hingga', ...   \n",
       "4                                    ['baik', 'tks']   \n",
       "\n",
       "                                  Data Teks_Stopword  \\\n",
       "0  ['selamat', 'malam', 'first', 'people', 'mohon...   \n",
       "1                    ['mohon', 'memeriksa', 'pesan']   \n",
       "2  ['hi', 'first', 'people', 'merespon', 'via', '...   \n",
       "3  ['estimasi', 'perbaikan', 'oktober', 'wib', 'i...   \n",
       "4                                            ['tks']   \n",
       "\n",
       "                                  Data Teks_Stemming  jumlah_polarity sentimen  \n",
       "0  selamat malam first people mohon maaf ketidakn...              -10  negatif  \n",
       "1                              mohon memeriksa pesan               -6  negatif  \n",
       "2  hi first people merespon via pesan silakan dic...                8  positif  \n",
       "3  estimasi perbaikan oktober wib informasi probl...               -5  negatif  \n",
       "4                                                tks                2  positif  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data\n",
    "data = pd.read_csv('E:\\Program\\[2] Program\\AnSent\\Data\\Data_5_Lex_sen\\Lex_Firstmediacares.csv', sep=(','))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_5132/2581358189.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  data = data[data_s.sentimen != 'netral']\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5132/2581358189.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# menghapus dataframe netral\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentimen\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'netral'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3447\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3448\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3449\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3451\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3500\u001b[0m         \u001b[1;31m# check_bool_indexer will throw exception if Series key cannot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3501\u001b[0m         \u001b[1;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3502\u001b[1;33m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3503\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2386\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2388\u001b[1;33m             raise IndexingError(\n\u001b[0m\u001b[0;32m   2389\u001b[0m                 \u001b[1;34m\"Unalignable boolean Series provided as \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2390\u001b[0m                 \u001b[1;34m\"indexer (index of the boolean Series and of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "# menghapus dataframe netral\n",
    "data = data[data_s.sentimen != 'netral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=data['Data Teks_Stemming']\n",
    "b=data['sentimen']\n",
    "\n",
    "a_train, a_test, b_train, b_test = train_test_split(A,b,test_size = 0.20, random_state = 42)\n",
    "\n",
    "print('Banyak data train :',len(a_train))\n",
    "print('Banyak data test  :',len(a_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelnb = GaussianNB()\n",
    "\n",
    "# #input data train pada fungsi klasifikasi NB\n",
    "# nbtrain = modelnb.fit(a_train, b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Menentukan hasil prediksi dari a_test\n",
    "# b_pred = nbtrain.predict(a_test, b_train)\n",
    "# b_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Menentukan probabilitas hasil prediksi\n",
    "# nbtrain.predict_proba(a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #confusion matrix\n",
    "# from sklearn.metrics import confusion\n",
    "\n",
    "# confusion_matrix(b_test, b_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Menerapkan hasil confusion matrix\n",
    "# y_actual = pd.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(b_test, b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon + KNN\n",
    "\n",
    "### Source CODE\n",
    "https://github.com/commitunuja/analisis-sentimen-naive-bayes-tf-idf/blob/master/klasifikasi_akurasi.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_lxn=TfidfVectorizer()\n",
    "clf1_lxn= KNeighborsClassifier()\n",
    "\n",
    "\n",
    "#Pipeline\n",
    "model_lxn = Pipeline([('vectorizer',tvec_lxn),\n",
    "                 ('classifier',clf1_lxn)])\n",
    "\n",
    "\n",
    "model_lxn.fit(a_test,b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_lxn = model_lxn.predict(a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(b_test, hasil_lxn))\n",
    "print(classification_report(hasil_lxn, b_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score: {:.2f}\".format((accuracy_score(hasil_lxn,b_test)*100)))\n",
    "print(\"Precision Score: {:.2f}\".format(precision_score(hasil_lxn, b_test, average='macro')))\n",
    "print(\"Recall Score: {:.2f}\".format(recall_score(hasil_lxn, b_test, average='macro')))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8da01e5a71448ea74f54d88afa8911010d1d12e23bc7e103d40d5def4a09152c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
